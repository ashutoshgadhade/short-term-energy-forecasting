{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from feature_engine.timeseries.forecasting import (\n",
    "    WindowFeatures,\n",
    "    ExpandingWindowFeatures,\n",
    "    LagFeatures\n",
    ")\n",
    "import pandas as pd\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from sktime.transformations.series.fourier import FourierFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from feature_engine.timeseries.forecasting import (\n",
    "    WindowFeatures,\n",
    "    ExpandingWindowFeatures,\n",
    "    LagFeatures\n",
    ")\n",
    "import pandas as pd\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from sktime.transformations.series.fourier import FourierFeatures\n",
    "\n",
    "def plot_errors(ErrorSeries):\n",
    "    \"\"\"\n",
    "    Plots the Mean Squared Error (MSE) values against features to visualize feature selection performance.\n",
    "\n",
    "    Args:\n",
    "        ErrorSeries (pd.Series): A pandas Series where indices are feature names\n",
    "                                 and values are corresponding errors (e.g., MSE).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Prepare the data\n",
    "    x = np.arange(len(ErrorSeries))  # Numerical x-axis indices\n",
    "    y = ErrorSeries.values           # Error values\n",
    "    labels = ErrorSeries.index       # Feature names\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")  # Use clean seaborn grid style\n",
    "\n",
    "    # Plot the error series\n",
    "    plt.plot(x, y, marker='o', linestyle='-', color='b', linewidth=2)\n",
    "\n",
    "    # Customize x-ticks and labels\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"MSE (Mean Squared Error)\", wrap=True)\n",
    "    plt.xlabel(\"Features\", wrap=True)\n",
    "    plt.title(\"Feature Selection Performance: MSE vs Features\", fontsize=14, pad=15)\n",
    "\n",
    "    # Adjust margins and spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature selection using SHAP-ordered features and TS Cross-Validation\n",
    "def FeatureSelection_fixed_test(regressor, DF_features, DF_target, ordered_features_list, test_size=672, tolerance=50):\n",
    "    feature_list = []\n",
    "    error_list = []\n",
    "    total_samples = len(DF_features)\n",
    "    n_splits = 5\n",
    "    no_improvement_count = 0\n",
    "    min_error = float('inf')\n",
    "\n",
    "    for i in ordered_features_list:\n",
    "        feature_list.append(i)\n",
    "        X = DF_features[feature_list].to_numpy()\n",
    "        y = DF_target.to_numpy()\n",
    "\n",
    "        # Custom TS CV logic\n",
    "        splits = []\n",
    "        start_train_size = total_samples - (n_splits * test_size)\n",
    "        for split in range(n_splits):\n",
    "            train_end = start_train_size + split * test_size\n",
    "            test_start = train_end\n",
    "            test_end = test_start + test_size\n",
    "            if test_end <= total_samples:\n",
    "                splits.append((list(range(0, train_end)), list(range(test_start, test_end))))\n",
    "\n",
    "        # Perform cross-validation\n",
    "        TimeSeriesCVerror = []\n",
    "        for train_index, test_index in splits:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            regressor.fit(X_train, y_train)\n",
    "            predicted_val = regressor.predict(X_test)\n",
    "            Error = mean_squared_error(y_test, predicted_val)\n",
    "            TimeSeriesCVerror.append(Error)\n",
    "\n",
    "        # Average error across splits\n",
    "        TS_CV_error = sum(TimeSeriesCVerror) / len(TimeSeriesCVerror)\n",
    "        error_list.append(TS_CV_error)\n",
    "\n",
    "        # Track improvement\n",
    "        if TS_CV_error < min_error:\n",
    "            min_error = TS_CV_error\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= tolerance:\n",
    "            print(f\"No improvement after {tolerance} features. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    ErrorSeries = pd.Series(error_list, index=feature_list)\n",
    "# Plot the errors using a custom plot function\n",
    "    plot_errors(ErrorSeries)\n",
    "\n",
    "\n",
    "    return ErrorSeries\n",
    "\n",
    "def keep_indices_till_min(series):\n",
    "    selected_features = []\n",
    "    min_value = series.min()\n",
    "    for idx, value in series.items():\n",
    "        if idx not in selected_features:\n",
    "            selected_features.append(idx)\n",
    "        if value == min_value:\n",
    "            break\n",
    "    return selected_features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def peak_RMSE(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 5000) -> float:\n",
    "    \"\"\"\n",
    "    Calculate RMSE for values where the true values exceed a certain threshold.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Array of true target values.\n",
    "        y_pred (np.ndarray): Array of predicted target values.\n",
    "        threshold (float): Threshold for filtering peak values (default is 5000).\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE for the filtered values.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    filtered_indices = y_true > threshold\n",
    "\n",
    "    # Filter both predictions and true values based on the threshold\n",
    "    filtered_predictions = y_pred[filtered_indices]\n",
    "    filtered_real_values = y_true[filtered_indices]\n",
    "\n",
    "    # Avoid division by zero or empty slices\n",
    "    if len(filtered_real_values) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate RMSE for the filtered data\n",
    "    rmse = np.sqrt(mean_squared_error(filtered_real_values, filtered_predictions))\n",
    "    return rmse\n",
    "\n",
    "def ErrorCalculator(name: str, y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 5000) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate multiple error metrics for predictions.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the pipeline or model.\n",
    "        y_true (np.ndarray): Array of true target values.\n",
    "        y_pred (np.ndarray): Array of predicted target values.\n",
    "        threshold (float): Threshold for peak RMSE calculation (default is 5000).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing RMSE, MAE, MSE, and PRMSE.\n",
    "    \"\"\"\n",
    "    errors = {\n",
    "        \"Pipelines\": name,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"PRMSE\": peak_RMSE(y_true, y_pred, threshold),\n",
    "    }\n",
    "    return errors\n",
    "\n",
    "\n",
    "def CreateWorkHourFeature(input_data):\n",
    "    \"\"\"\n",
    "    Receives as input a DataFrame or Series and outputs a DataFrame with the working hours during the day.\n",
    "    When the day of the week is larger than 4, it is considered a weekend (1), otherwise, it's a workday (0).\n",
    "    During workdays and between 8:00 and 17:00, it is considered a working hour.\n",
    "\n",
    "    Parameters:\n",
    "    input_data (DataFrame or Series): Input data with a DatetimeIndex.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the added \"WorkingHour_flag\" column.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, pd.Series):\n",
    "        input_df = pd.DataFrame(input_data)\n",
    "    elif isinstance(input_data, pd.DataFrame):\n",
    "        input_df = input_data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a DataFrame or Series.\")\n",
    "\n",
    "    assert isinstance(input_df.index, pd.DatetimeIndex), \"Index must be a datetime index.\"\n",
    "\n",
    "    input_df[\"dayOfWeek\"] = input_df.index.dayofweek\n",
    "    input_df.loc[input_df[\"dayOfWeek\"] > 4, \"weekendFlag\"] = 1\n",
    "    input_df.loc[input_df[\"dayOfWeek\"] < 5, \"weekendFlag\"] = 0\n",
    "    input_df[\"hour\"] = input_df.index.hour\n",
    "    input_df[\"WorkingHour_flag\"] = 0\n",
    "    input_df.loc[((input_df[\"hour\"] > 8) & (input_df[\"hour\"] < 17) & (input_df[\"weekendFlag\"] == 0)), \"WorkingHour_flag\"] = 1\n",
    "    input_df.drop([\"hour\", \"dayOfWeek\", \"weekendFlag\"], axis=1, inplace=True)\n",
    "\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def ListCreatorFlagger(df, substrings=['flag', 'cos', 'sin','day_of_week', 'day_of_month', 'weekend', 'days_in_month', 'hour', 'minute']):\n",
    "    \"\"\"\n",
    "    A function that separates the columns containing specified substrings from those that don't.\n",
    "    df is the dataframe in question and the substring is a list.\n",
    "    \"\"\"\n",
    "    flag_columns = [col for col in df.columns if any(substring in col for substring in substrings)]\n",
    "\n",
    "    if not flag_columns:\n",
    "        print(\"No columns with the specified substrings found.\")\n",
    "        return None, None\n",
    "\n",
    "    non_flag_columns = [col for col in df.columns if col not in flag_columns]\n",
    "\n",
    "    return non_flag_columns, flag_columns\n",
    "\n",
    "def HolidayFeatureCreator(input_data):\n",
    "    \"\"\"\n",
    "    Receives as input a DataFrame or Series and creates a column named \"Holidays_flag\" with 1 if there is a holiday and with 0 if no holidays exist.\n",
    "    Holidays derived from Germany.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, pd.Series):\n",
    "        input_df = pd.DataFrame(input_data)\n",
    "    elif isinstance(input_data, pd.DataFrame):\n",
    "        input_df = input_data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a DataFrame or Series.\")\n",
    "\n",
    "    assert isinstance(input_df.index, pd.DatetimeIndex), \"Index must be a datetime index.\"\n",
    "\n",
    "    national_holidays_all = holidays.DE(years=[2014,2015,2016,2017,2018,2019,2020, 2021, 2022, 2023, 2024, 2025, 2026]).items()\n",
    "    national_holidays = [items[0] for items in national_holidays_all]  # this is a list\n",
    "\n",
    "    # Create a new column for holidays flag\n",
    "    input_df[\"Holidays_flag\"] = 0\n",
    "\n",
    "    # Iterate over the index and set holiday flag to 1 if the date matches any national holiday\n",
    "    for index_date in input_df.index:\n",
    "        if index_date.date() in national_holidays:\n",
    "            input_df.at[index_date, \"Holidays_flag\"] = 1\n",
    "\n",
    "    return input_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def TimeRelatedFeatureConstructor(df):\n",
    "  \"\"\"\n",
    "  Works only in a dataframe as input: run the other functions first.\n",
    "  Extracts time-related features\n",
    "  \"\"\"\n",
    "  TimeFeaturesToExtract=[\"day_of_week\",\"weekend\",\"hour\",\"minute\",] #consider to add more\n",
    "  dtfs=DatetimeFeatures(variables=\"index\", features_to_extract=TimeFeaturesToExtract, drop_original=False)\n",
    "  df=dtfs.fit_transform(df)\n",
    "\n",
    "  CyclicalFeaturesToExtract=[\"day_of_week\",\"hour\",\"minute\",]\n",
    "  cyclical_dtfs=CyclicalFeatures(variables=CyclicalFeaturesToExtract,drop_original=False)\n",
    "  df=cyclical_dtfs.fit_transform(df)\n",
    "  return df\n",
    "\n",
    "\n",
    "def FourierFeatureConstructor(df,granularity,fourier_terms_list):\n",
    "\n",
    "    number_part = ''.join(filter(str.isdigit, granularity))\n",
    "    number_int = int(number_part)\n",
    "    minutes4hour=60/number_int\n",
    "\n",
    "    Fourier_Transformer=FourierFeatures(\n",
    "        sp_list=[minutes4hour, 24*minutes4hour,24*7*minutes4hour,24*30*minutes4hour],   # hourly, 24 is daily seasonality *12 because for 60 min we have 12 intervals,  and 24*7 is weekly seasonality\n",
    "        fourier_terms_list=fourier_terms_list,\n",
    "        freq=granularity, #not necessery\n",
    "        keep_original_columns=True,\n",
    "\n",
    "    )\n",
    "\n",
    "    Fourier_Transformer.fit(df)\n",
    "    df=Fourier_Transformer.transform(df)\n",
    "    return df\n",
    "\n",
    "def WindowFeaturesConstructor(df, granularity, ListWithNoFlags):\n",
    "    \"\"\"\n",
    "    This is a function that makes a list of 4 window features starting from double the granularity and following by doubling the previous value\n",
    "    \"\"\"\n",
    "    number_part = ''.join(filter(str.isdigit, granularity))\n",
    "    number_int = int(number_part)\n",
    "    double_granularity = 2 * number_int\n",
    "    time_intervals = [double_granularity]\n",
    "\n",
    "    # Calculate subsequent values\n",
    "    for i in range(3):\n",
    "        time_intervals.append(time_intervals[-1] * 2)\n",
    "\n",
    "    windowlist = [interval // number_int for interval in time_intervals]  # Corrected division\n",
    "    functionsList = [\"mean\", \"std\"]\n",
    "    WindownFeatureTransformer = WindowFeatures(variables=ListWithNoFlags,\n",
    "                                               functions=functionsList,\n",
    "                                               window=windowlist,\n",
    "                                               freq=granularity,\n",
    "                                               drop_original=False)\n",
    "\n",
    "    df = WindownFeatureTransformer.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "def ExpandingWindowFeatureConstructor(df,ListWithNoFlags):\n",
    "  functionsList=[\"mean\",\"std\"]\n",
    "  frequency = pd.infer_freq(df.index) #infer the frequency from the dataframe\n",
    "  ExpandingWindownFeatureTransformer=ExpandingWindowFeatures(variables=ListWithNoFlags,\n",
    "                                                           functions=functionsList,\n",
    "                                                           freq=frequency, #I put the freq to shift it down! but now it is performed automatically!\n",
    "                                                           drop_original=False)\n",
    "  df=ExpandingWindownFeatureTransformer.fit_transform(df)\n",
    "  return df\n",
    "\n",
    "def WeightedLinearFeatureMaker(df,ListWithNoFlags,granularity):\n",
    "  \"\"\"\n",
    "  This is a function that takes the original DF and modifies the continious value columns\n",
    "  Inputs: Dataframe, List of columns that are continous values, daily window to slide, weights of the values\n",
    "  \"\"\"\n",
    "  number_part = ''.join(filter(str.isdigit, granularity))\n",
    "  Minutedensity=int(number_part)\n",
    "  Window=int((60/Minutedensity)*24) #288 means a daily window\n",
    "  weights=np.arange(1,Window+1)\n",
    "\n",
    "  # if i had hourly data then i would have had np.arange(1,24*7) for a weekly window\n",
    "\n",
    "  def weighted_mean (x,weights):\n",
    "    return (weights*x).sum()/weights.sum()\n",
    "\n",
    "  def weighted_std(x,weights):\n",
    "    mean_w= weighted_mean(x, weights)\n",
    "    var_w= (weights* (x-mean_w)**2).sum()/weights.sum()\n",
    "    return np.sqrt(var_w)\n",
    "\n",
    "  # LETS make the weighted mean column\n",
    "  for i in ListWithNoFlags:\n",
    "    result=(\n",
    "        df[i]\n",
    "        .rolling(window=Window) #here we pick a window size. Needs to be the same as the len(weights)\n",
    "        .apply(weighted_mean, args=(weights,))\n",
    "        .shift(1)#shift by 1 to avoid data leakage\n",
    "        .to_frame()#convert series to df\n",
    "        )\n",
    "\n",
    "    result.columns=[str(i)+\"_weighted_\"+str(Window)+\"_mean\"]\n",
    "    df=df.join(result)\n",
    "\n",
    "  for i in ListWithNoFlags:\n",
    "    result=(\n",
    "        df[i]\n",
    "        .rolling(window=Window) #here we pick a window size. Needs to be the same as the len(weights)\n",
    "        .apply(weighted_std, args=(weights,))\n",
    "        .shift(1)#shift by 1 to avoid data leakage\n",
    "        .to_frame()#convert series to df\n",
    "        )\n",
    "\n",
    "    result.columns=[str(i)+\"_weighted_\"+str(Window)+\"_std\"]\n",
    "    df=df.join(result)\n",
    "  return df\n",
    "\n",
    "def ExpWeightMeanMaker(df,ListWithNoFlags,granularity):\n",
    "  \"\"\"\n",
    "  This is a function that makes exp weighted average with a sliding window approach\n",
    "  \"\"\"\n",
    "  number_part = ''.join(filter(str.isdigit, granularity))\n",
    "  Minutedensity=int(number_part)\n",
    "  Window=int((60/Minutedensity)*24) #288 means a daily window\n",
    "\n",
    "  def exp_weights(alpha,window_size):\n",
    "    \"\"\"\n",
    "    a function to calculate the weights for every single component of our sliding windown\n",
    "    \"\"\"\n",
    "    weights=np.ones(window_size) #initializing weights\n",
    "    for ix in range(window_size):\n",
    "      weights[ix]=(1-alpha)**(window_size-1-ix)\n",
    "    return weights\n",
    "\n",
    "  def exp_weighted_mean(x):\n",
    "    \"\"\"\n",
    "    a functions that calculates the exp weigted mean\n",
    "    \"\"\"\n",
    "\n",
    "    weights=exp_weights(alpha=0.05, window_size=len(x)) # HERE WE SET THE ALPHA\n",
    "    return (weights*x).sum()/weights.sum()\n",
    "\n",
    "  for i in ListWithNoFlags:\n",
    "    result=(\n",
    "        df[i]\n",
    "        .rolling(window=int(Window))\n",
    "        .agg([exp_weighted_mean])\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    result.columns=[str(i)+\"_Exp_weighted_\"+str(Window)+\"_SL.win\"]\n",
    "    df=df.join(result)\n",
    "  return df\n",
    "\n",
    "def WeightedExponentialExpandingWindow(df,ListWithNoFlags,alpha):\n",
    "  \"\"\"\n",
    "  This is a funtion that takes as input the df,a list of continuous values and the alpha.\n",
    "  Outputs: all continuous features on the df that are \"mean\" and \"std\"\n",
    "  \"\"\"\n",
    "\n",
    "  for i in ListWithNoFlags:\n",
    "    df[[str(i)+\"_ewm_mean_expanding.win\",str(i)+\"ewm_std_expanding.win\"]]= (\n",
    "                                              df[i].ewm(alpha=alpha).\n",
    "                                              agg([\"mean\",\"std\"])\n",
    "                                              .shift(1)\n",
    "                                            )\n",
    "  return df\n",
    "\n",
    "def FeatureLagger(df,ListOfFeatures,granularity,PredictionHorizon):\n",
    "\n",
    "    time_intervals = []\n",
    "    number_part = ''.join(filter(str.isdigit, granularity))\n",
    "    Minutedensity=int(number_part)\n",
    "    end_in_day=int((PredictionHorizon)/(Minutedensity))\n",
    "    for i in range(1, 1+end_in_day):  # 24 hours * 60 minutes / 15 minutes = 96 intervals\n",
    "        time_intervals.append(f\"{i * 15}min\")\n",
    "\n",
    "    lag_transformer= LagFeatures(variables=ListOfFeatures,\n",
    "                                freq=time_intervals,\n",
    "                                drop_original=False) #make a lagger transformer drop all original features\n",
    "\n",
    "    df=lag_transformer.fit_transform(df) # transform the features to DF joined\n",
    "    return df\n",
    "def separate_future_past_features(df_columns):\n",
    "    \"\"\"\n",
    "    Separates future and past features from a list of dataframe columns.\n",
    "\n",
    "    Args:\n",
    "        df_columns (list): A list of column names from the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'future_features' and 'past_features', containing the respective lists of column names.\n",
    "    \"\"\"\n",
    "    future_keywords = ['sin', 'cos', 'weekend', 'hour', 'holiday', 'minute', 'day','+']\n",
    "    \n",
    "    future_features = []\n",
    "    past_features = []\n",
    "    \n",
    "    for col in df_columns:\n",
    "        # Check if the column contains \"+\" in its name to classify as a past feature\n",
    "        if any(keyword in col.lower() for keyword in future_keywords):\n",
    "            future_features.append(col)\n",
    "        # Columns that don't meet the above conditions are considered past features by default\n",
    "        else:\n",
    "            past_features.append(col)\n",
    "\n",
    "    return future_features, past_features\n",
    "\n",
    "def plot_predictions(model_name, df_validation_y, all_preds_unscaled):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot actual values\n",
    "    plt.plot(df_validation_y.index, df_validation_y.values, label='Actual Values', color='blue', linewidth=2)\n",
    "    \n",
    "    # Plot predicted values\n",
    "    plt.plot(all_preds_unscaled.index, all_preds_unscaled.values, label=f'Predicted Values ({model_name})', color='red', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Actual vs Predicted Values ({model_name})')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-ticks for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DF:-\n",
    "file = r\"C:\\Users\\gkeep\\OneDrive\\Desktop\\Thesis_coding\\Building_Load_15min_Kw.csv\"  # Replace with your file name\n",
    "df = pd.read_csv(file,index_col=0, parse_dates=[0])\n",
    "df=df[df.index.year.isin([2013])]\n",
    "df=df[df.index.month.isin([1,2,3,4,5,6])]\n",
    "\n",
    "\n",
    "# Step 1: Convert \"Unnamed: 0\" to a proper datetime format\n",
    "#df.index= pd.to_datetime(df.index)\n",
    "\n",
    "# Define parameters for processing\n",
    "OriginalFeatures = [\"Temp (C)\", \"Dew Point Temp (C)\", \"Rel Hum (%)\", \"Wind Dir (10s deg)\" , \"Stn Press (kPa)\" , \"Wind Spd (km/h)\" , \"Visibility (km)\" , \"WHE (kW)\" ]\n",
    "target = \"WHE (kW)\"\n",
    "Minutedensity = 15\n",
    "granularity = \"15min\"\n",
    "PredictionHorizon=prediction_horizon = 1440 # in minutes\n",
    "fourier_terms_list = [2, 2, 2, 2]\n",
    "\n",
    "# Create Features:-\n",
    "df = CreateWorkHourFeature(df)\n",
    "df = HolidayFeatureCreator(df)\n",
    "non_flag_columns, flag_columns = ListCreatorFlagger(df)\n",
    "print(f\"Non-flag columns: {non_flag_columns}\")\n",
    "print(f\"Flag columns: {flag_columns}\")\n",
    "df = FeatureLagger(df, ListOfFeatures=non_flag_columns, granularity=granularity, PredictionHorizon=prediction_horizon)\n",
    "# Add Fourier features\n",
    "df = FourierFeatureConstructor(df, granularity=granularity, fourier_terms_list=fourier_terms_list)\n",
    "# Add window features\n",
    "df = WindowFeaturesConstructor(df, granularity=granularity, ListWithNoFlags=non_flag_columns)\n",
    "# Add time-related features\n",
    "df = TimeRelatedFeatureConstructor(df)\n",
    "\n",
    "\n",
    "df[target+\"_+lag\"+str(PredictionHorizon)]=df[target].shift(int(-PredictionHorizon/Minutedensity))\n",
    "target=target+\"_+lag\"+str(PredictionHorizon)\n",
    "df.dropna(inplace=True)\n",
    "has_nan = df.isnull().values.any()\n",
    "print(f\"Does the DataFrame have NaN values? {has_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_after_FS1_df = pd.read_csv(r'C:\\Users\\gkeep\\OneDrive\\Desktop\\Thesis_coding\\Selected_features_18.02.25_1-6.csv')\n",
    "features_after_FS1 = features_after_FS1_df[\"Features\"].tolist()\n",
    "features_after_FS1.append(target)\n",
    "features_after_FS1\n",
    "df = df[features_after_FS1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Determine the number of rows in the DataFrame\n",
    "total_size = len(df)\n",
    "\n",
    "# Step 2: Define the number of rows for validation and test sets\n",
    "validation_test_size = 96 * 7 * 2\n",
    "\n",
    "# Step 3: Split the data\n",
    "DF_training = df.iloc[:total_size - 2 * validation_test_size]\n",
    "DF_validation = df.iloc[total_size - 2 * validation_test_size:total_size - validation_test_size]\n",
    "DF_test = df.iloc[total_size - validation_test_size:]\n",
    "\n",
    "# Step 4: Verify the sizes of each set\n",
    "print(\"Training set size:\", len(DF_training))\n",
    "print(\"Validation set size:\", len(DF_validation))\n",
    "print(\"Test set size:\", len(DF_test))\n",
    "\n",
    "X_train, y_train = DF_training.drop(columns=['Demand MW_+lag1440']), DF_training['Demand MW_+lag1440']\n",
    "x_validation, y_validation = DF_validation.drop(columns=['Demand MW_+lag1440']), DF_validation['Demand MW_+lag1440']\n",
    "X_test, y_test = DF_test.drop(columns=['Demand MW_+lag1440']), DF_test['Demand MW_+lag1440']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-2688:]\n",
    "\n",
    "df_1=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TCN_28.02_n=100_256\\TCN_final_predictions.csv') # Validation Predictions\n",
    "df_2=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TEST\\TCN_07.03_Test_Results\\TCN_final_predictions.csv')         # Test predictions\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "df[\"Target\"] = new_df[\"TCN_unscaled\"].values  # Ensure values are aligned\n",
    "\n",
    "df=df.drop(columns=['WHE (kW)_+lag1440'])\n",
    "\n",
    "DF_test = df.iloc[-1344:]\n",
    "\n",
    "## Normal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------\n",
    "# User-defined paths\n",
    "# ----------------------\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\TCN\\ElasticNet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Load or define your data\n",
    "# ----------------------\n",
    "# Replace with your actual data\n",
    "# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=[0])\n",
    "# For example:\n",
    "# df = pd.read_csv(\"my_dataset.csv\")\n",
    "# DF_test = pd.read_csv(\"my_testset.csv\")\n",
    "\n",
    "# Placeholder (make sure to replace!)\n",
    "# df, DF_test must contain \"Target\" column\n",
    "#assert 'df' in globals() and 'DF_test' in globals(), \"Load your dataset as df and DF_test first.\"\n",
    "\n",
    "X_train = df.drop(columns=['Target'])\n",
    "y_train = df['Target']\n",
    "X_test = DF_test.drop(columns=['Target'])\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ----------------------\n",
    "# Scaling\n",
    "# ----------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------\n",
    "# Optuna Optimization\n",
    "# ----------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 10.0),\n",
    "        \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**params, max_iter=10000, random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mse_scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# ----------------------\n",
    "# Train on full training data\n",
    "# ----------------------\n",
    "final_model = ElasticNet(**best_params, max_iter=10000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Save predictions\n",
    "# ----------------------\n",
    "pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}, index=y_test.index)\n",
    "pred_df.to_csv(os.path.join(output_dir, \"ElasticNet_predictions.csv\"))\n",
    "\n",
    "# ----------------------\n",
    "# Save results\n",
    "# ----------------------\n",
    "results = {\n",
    "    \"Test MSE\": test_mse,\n",
    "    \"Best Hyperparameters\": best_params\n",
    "}\n",
    "with open(os.path.join(output_dir, \"ElasticNet_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance Plot\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train.columns, np.abs(final_model.coef_))\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"ElasticNet Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Plot Predictions\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual\", linewidth=1)\n",
    "plt.plot(y_test.index, y_pred, label=\"ElasticNet Prediction\", color=\"red\", linewidth=1)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"ElasticNet Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target']) # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target']) # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    results[model_name] = mse_test\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Demand MW_+lag1440\")\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted Demand MW_+lag1440 ({model_name})\", color='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Demand MW_+lag1440\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, mse in results.items():\n",
    "    print(f\"{model}: Test MSE = {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\TCN\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target'])  # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target'])  # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_test}, index=y_test.index)\n",
    "    pred_csv_path = os.path.join(output_dir, f\"{model_name}_predictions.csv\")\n",
    "    pred_df.to_csv(pred_csv_path, index=True)\n",
    "    \n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        \"Test MSE\": mse_test,\n",
    "        \"Best Hyperparameters\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Predictions\" , linewidth=1)\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted with ({model_name})\", color='red' , linestyle='-' , linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Save all results to JSON file\n",
    "results_json_path = os.path.join(output_dir, \"summary_results.json\")\n",
    "with open(results_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"{model}: Test MSE = {data['Test MSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target']) # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target']) # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ðŸš¨ **Ensure NO Data Leakage: Fit scaler ONLY on training set**\n",
    "scaler = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Fit the feature scaler only on training set\n",
    "scaler.fit(X_train)  # Fit on training features only\n",
    "\n",
    "# Scale features for both training and test sets using the fitted scaler\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# ðŸš¨ **Fit the target scaler ONLY on the original (unscaled) training target values**\n",
    "scaler_target.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform target variable separately for training and test sets\n",
    "y_train_scaled = pd.DataFrame(scaler_target.transform(y_train.values.reshape(-1, 1)), columns=['Target'])\n",
    "y_test_scaled = pd.DataFrame(scaler_target.transform(y_test.values.reshape(-1, 1)), columns=['Target'])\n",
    "\n",
    "# ðŸ”¹ Print shapes to verify everything is correct\n",
    "print(f\"X_train: {X_train.shape}, X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, X_test_scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "# ðŸ”Ž **Final Sanity Check**\n",
    "print(\"Training Target Min/Max (Scaled):\", y_train_scaled.min().values[0], y_train_scaled.max().values[0])\n",
    "print(\"Test Target Min/Max (Scaled):\", y_test_scaled.min().values[0], y_test_scaled.max().values[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Define Optuna objective function for Lasso hyperparameter tuning\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)  # Search for best alpha (log scale)\n",
    "    \n",
    "    # Train Lasso model on TRAINING SET ONLY\n",
    "    model = Lasso(alpha=alpha, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "    # Predict on training set (for Optuna optimization)\n",
    "    y_pred_train_scaled = model.predict(X_train_scaled)\n",
    "    y_pred_train = scaler_target.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) on training set\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    \n",
    "    return mse_train  # Optuna will minimize training MSE\n",
    "\n",
    "# Run Optuna study to find the best hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize training MSE\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)  # Run 50 trials in parallel\n",
    "\n",
    "# Best hyperparameters from Optuna\n",
    "best_alpha = study.best_params[\"alpha\"]\n",
    "print(\"Best Hyperparameters (Lasso):\", study.best_params)\n",
    "\n",
    "# Train the final Lasso model using the best alpha on FULL TRAINING DATA (X_train_scaled)\n",
    "best_lasso_model = Lasso(alpha=best_alpha, random_state=42)\n",
    "best_lasso_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict on test set using the final trained model\n",
    "y_pred_test_scaled = best_lasso_model.predict(X_test_scaled)\n",
    "y_pred_test = scaler_target.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate MSE on test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "# ðŸ”¹ Feature Importance Analysis (Lasso Coefficients)\n",
    "feature_importances = np.abs(best_lasso_model.coef_)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train_scaled.columns, feature_importances, align='center')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Optimized Lasso Feature Importance Analysis (Optuna)\")\n",
    "plt.show()\n",
    "\n",
    "# ðŸ”¹ Plot Actual vs. Predicted Values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual Demand MW_+lag1440\")\n",
    "plt.plot(y_test.index, y_pred_test, label=\"Predicted Demand MW_+lag1440 (Lasso)\", color='red')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Demand MW_+lag1440\")\n",
    "plt.title(\"Lasso Time-Series Forecasting with Optuna\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSmixersx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-2688:]\n",
    "\n",
    "df_1=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TSMixerx_01.03_n=100_256\\TSMixerx_final_predictions.csv') # Validation Predictions\n",
    "df_2=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TEST\\TSMixerx_11.03_Test_Results\\TSMixerx_final_predictions.csv')         # Test predictions\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "df[\"Target\"] = new_df[\"TSMixerx_unscaled\"].values  # Ensure values are aligned\n",
    "\n",
    "df=df.drop(columns=['WHE (kW)_+lag1440'])\n",
    "\n",
    "DF_test = df.iloc[-1344:]\n",
    "\n",
    "## Normal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------\n",
    "# User-defined paths\n",
    "# ----------------------\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\TSMixerx\\ElasticNet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Load or define your data\n",
    "# ----------------------\n",
    "# Replace with your actual data\n",
    "# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=[0])\n",
    "# For example:\n",
    "# df = pd.read_csv(\"my_dataset.csv\")\n",
    "# DF_test = pd.read_csv(\"my_testset.csv\")\n",
    "\n",
    "# Placeholder (make sure to replace!)\n",
    "# df, DF_test must contain \"Target\" column\n",
    "#assert 'df' in globals() and 'DF_test' in globals(), \"Load your dataset as df and DF_test first.\"\n",
    "\n",
    "X_train = df.drop(columns=['Target'])\n",
    "y_train = df['Target']\n",
    "X_test = DF_test.drop(columns=['Target'])\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ----------------------\n",
    "# Scaling\n",
    "# ----------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------\n",
    "# Optuna Optimization\n",
    "# ----------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 10.0),\n",
    "        \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**params, max_iter=10000, random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mse_scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# ----------------------\n",
    "# Train on full training data\n",
    "# ----------------------\n",
    "final_model = ElasticNet(**best_params, max_iter=10000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Save predictions\n",
    "# ----------------------\n",
    "pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}, index=y_test.index)\n",
    "pred_df.to_csv(os.path.join(output_dir, \"ElasticNet_predictions.csv\"))\n",
    "\n",
    "# ----------------------\n",
    "# Save results\n",
    "# ----------------------\n",
    "results = {\n",
    "    \"Test MSE\": test_mse,\n",
    "    \"Best Hyperparameters\": best_params\n",
    "}\n",
    "with open(os.path.join(output_dir, \"ElasticNet_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance Plot\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train.columns, np.abs(final_model.coef_))\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"ElasticNet Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Plot Predictions\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual\", linewidth=1)\n",
    "plt.plot(y_test.index, y_pred, label=\"ElasticNet Prediction\", color=\"red\", linewidth=1)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"ElasticNet Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\TSMixerx\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target'])  # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target'])  # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_test}, index=y_test.index)\n",
    "    pred_csv_path = os.path.join(output_dir, f\"{model_name}_predictions.csv\")\n",
    "    pred_df.to_csv(pred_csv_path, index=True)\n",
    "    \n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        \"Test MSE\": mse_test,\n",
    "        \"Best Hyperparameters\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Predictions\" , linewidth=1)\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted with ({model_name})\", color='red' , linestyle='-' , linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Save all results to JSON file\n",
    "results_json_path = os.path.join(output_dir, \"summary_results.json\")\n",
    "with open(results_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"{model}: Test MSE = {data['Test MSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBEATSx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-2688:]\n",
    "\n",
    "df_1=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\Trial\\NBEATSx_15.05_n=100\\NBEATSx_final_predictions.csv') # Validation Predictions\n",
    "df_2=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TEST\\NBEATSx_19.05_Test_Results\\NBEATSx_final_predictions.csv')         # Test predictions\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "df[\"Target\"] = new_df[\"NBEATSx_unscaled\"].values  # Ensure values are aligned\n",
    "\n",
    "df=df.drop(columns=['WHE (kW)_+lag1440'])\n",
    "\n",
    "DF_test = df.iloc[-1344:]\n",
    "\n",
    "## Normal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------\n",
    "# User-defined paths\n",
    "# ----------------------\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\NBEATSx\\ElasticNet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Load or define your data\n",
    "# ----------------------\n",
    "# Replace with your actual data\n",
    "# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=[0])\n",
    "# For example:\n",
    "# df = pd.read_csv(\"my_dataset.csv\")\n",
    "# DF_test = pd.read_csv(\"my_testset.csv\")\n",
    "\n",
    "# Placeholder (make sure to replace!)\n",
    "# df, DF_test must contain \"Target\" column\n",
    "#assert 'df' in globals() and 'DF_test' in globals(), \"Load your dataset as df and DF_test first.\"\n",
    "\n",
    "X_train = df.drop(columns=['Target'])\n",
    "y_train = df['Target']\n",
    "X_test = DF_test.drop(columns=['Target'])\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ----------------------\n",
    "# Scaling\n",
    "# ----------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------\n",
    "# Optuna Optimization\n",
    "# ----------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 10.0),\n",
    "        \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**params, max_iter=10000, random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mse_scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# ----------------------\n",
    "# Train on full training data\n",
    "# ----------------------\n",
    "final_model = ElasticNet(**best_params, max_iter=10000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Save predictions\n",
    "# ----------------------\n",
    "pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}, index=y_test.index)\n",
    "pred_df.to_csv(os.path.join(output_dir, \"ElasticNet_predictions.csv\"))\n",
    "\n",
    "# ----------------------\n",
    "# Save results\n",
    "# ----------------------\n",
    "results = {\n",
    "    \"Test MSE\": test_mse,\n",
    "    \"Best Hyperparameters\": best_params\n",
    "}\n",
    "with open(os.path.join(output_dir, \"ElasticNet_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance Plot\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train.columns, np.abs(final_model.coef_))\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"ElasticNet Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Plot Predictions\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual\", linewidth=1)\n",
    "plt.plot(y_test.index, y_pred, label=\"ElasticNet Prediction\", color=\"red\", linewidth=1)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"ElasticNet Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\NBEATSx\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target'])  # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target'])  # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_test}, index=y_test.index)\n",
    "    pred_csv_path = os.path.join(output_dir, f\"{model_name}_predictions.csv\")\n",
    "    pred_df.to_csv(pred_csv_path, index=True)\n",
    "    \n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        \"Test MSE\": mse_test,\n",
    "        \"Best Hyperparameters\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Predictions\" , linewidth=1)\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted with ({model_name})\", color='red' , linestyle='-' , linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Save all results to JSON file\n",
    "results_json_path = os.path.join(output_dir, \"summary_results.json\")\n",
    "with open(results_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"{model}: Test MSE = {data['Test MSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-2688:]\n",
    "\n",
    "df_1=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\LSTM_13.03_n=100\\LSTM_final_predictions.csv') # Validation Predictions\n",
    "df_2=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TEST\\LSTM_14.03_Test_Results\\LSTM_final_predictions.csv')         # Test predictions\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "df[\"Target\"] = new_df[\"LSTM_unscaled\"].values  # Ensure values are aligned\n",
    "\n",
    "df=df.drop(columns=['WHE (kW)_+lag1440'])\n",
    "\n",
    "DF_test = df.iloc[-1344:]\n",
    "\n",
    "## Normal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------\n",
    "# User-defined paths\n",
    "# ----------------------\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\LSTM\\ElasticNet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Load or define your data\n",
    "# ----------------------\n",
    "# Replace with your actual data\n",
    "# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=[0])\n",
    "# For example:\n",
    "# df = pd.read_csv(\"my_dataset.csv\")\n",
    "# DF_test = pd.read_csv(\"my_testset.csv\")\n",
    "\n",
    "# Placeholder (make sure to replace!)\n",
    "# df, DF_test must contain \"Target\" column\n",
    "#assert 'df' in globals() and 'DF_test' in globals(), \"Load your dataset as df and DF_test first.\"\n",
    "\n",
    "X_train = df.drop(columns=['Target'])\n",
    "y_train = df['Target']\n",
    "X_test = DF_test.drop(columns=['Target'])\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ----------------------\n",
    "# Scaling\n",
    "# ----------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------\n",
    "# Optuna Optimization\n",
    "# ----------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 10.0),\n",
    "        \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**params, max_iter=10000, random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mse_scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# ----------------------\n",
    "# Train on full training data\n",
    "# ----------------------\n",
    "final_model = ElasticNet(**best_params, max_iter=10000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Save predictions\n",
    "# ----------------------\n",
    "pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}, index=y_test.index)\n",
    "pred_df.to_csv(os.path.join(output_dir, \"ElasticNet_predictions.csv\"))\n",
    "\n",
    "# ----------------------\n",
    "# Save results\n",
    "# ----------------------\n",
    "results = {\n",
    "    \"Test MSE\": test_mse,\n",
    "    \"Best Hyperparameters\": best_params\n",
    "}\n",
    "with open(os.path.join(output_dir, \"ElasticNet_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance Plot\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train.columns, np.abs(final_model.coef_))\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"ElasticNet Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Plot Predictions\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual\", linewidth=1)\n",
    "plt.plot(y_test.index, y_pred, label=\"ElasticNet Prediction\", color=\"red\", linewidth=1)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"ElasticNet Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\LSTM\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target'])  # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target'])  # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_test}, index=y_test.index)\n",
    "    pred_csv_path = os.path.join(output_dir, f\"{model_name}_predictions.csv\")\n",
    "    pred_df.to_csv(pred_csv_path, index=True)\n",
    "    \n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        \"Test MSE\": mse_test,\n",
    "        \"Best Hyperparameters\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Predictions\" , linewidth=1)\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted with ({model_name})\", color='red' , linestyle='-' , linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Save all results to JSON file\n",
    "results_json_path = os.path.join(output_dir, \"summary_results.json\")\n",
    "with open(results_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"{model}: Test MSE = {data['Test MSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-2688:]\n",
    "\n",
    "df_1=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\KAN_11.03_n=100\\KAN_final_predictions.csv') # Validation Predictions\n",
    "df_2=pd.read_csv(r'C:\\Users\\gkeep\\Desktop\\Results\\DNN\\TEST\\KAN_14.03_Test_Results\\KAN_final_predictions.csv')         # Test predictions\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "df[\"Target\"] = new_df[\"KAN_unscaled\"].values  # Ensure values are aligned\n",
    "\n",
    "df=df.drop(columns=['WHE (kW)_+lag1440'])\n",
    "\n",
    "DF_test = df.iloc[-1344:]\n",
    "\n",
    "## Normal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------\n",
    "# User-defined paths\n",
    "# ----------------------\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\KAN\\ElasticNet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Load or define your data\n",
    "# ----------------------\n",
    "# Replace with your actual data\n",
    "# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=[0])\n",
    "# For example:\n",
    "# df = pd.read_csv(\"my_dataset.csv\")\n",
    "# DF_test = pd.read_csv(\"my_testset.csv\")\n",
    "\n",
    "# Placeholder (make sure to replace!)\n",
    "# df, DF_test must contain \"Target\" column\n",
    "#assert 'df' in globals() and 'DF_test' in globals(), \"Load your dataset as df and DF_test first.\"\n",
    "\n",
    "X_train = df.drop(columns=['Target'])\n",
    "y_train = df['Target']\n",
    "X_test = DF_test.drop(columns=['Target'])\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# ----------------------\n",
    "# Scaling\n",
    "# ----------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------\n",
    "# Optuna Optimization\n",
    "# ----------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 10.0),\n",
    "        \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**params, max_iter=10000, random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mse_scores.append(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# ----------------------\n",
    "# Train on full training data\n",
    "# ----------------------\n",
    "final_model = ElasticNet(**best_params, max_iter=10000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Save predictions\n",
    "# ----------------------\n",
    "pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}, index=y_test.index)\n",
    "pred_df.to_csv(os.path.join(output_dir, \"ElasticNet_predictions.csv\"))\n",
    "\n",
    "# ----------------------\n",
    "# Save results\n",
    "# ----------------------\n",
    "results = {\n",
    "    \"Test MSE\": test_mse,\n",
    "    \"Best Hyperparameters\": best_params\n",
    "}\n",
    "with open(os.path.join(output_dir, \"ElasticNet_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance Plot\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(X_train.columns, np.abs(final_model.coef_))\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"ElasticNet Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Plot Predictions\n",
    "# ----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual\", linewidth=1)\n",
    "plt.plot(y_test.index, y_pred, label=\"ElasticNet Prediction\", color=\"red\", linewidth=1)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"ElasticNet Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\KAN\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training and test sets\n",
    "X_train = df.drop(columns=['Target'])  # Train on Validation + Test\n",
    "y_train = df['Target']\n",
    "\n",
    "X_test = DF_test.drop(columns=['Target'])  # Predict on Test\n",
    "y_test = DF_test['Target']\n",
    "\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"num_leaves\": trial.suggest_int('num_leaves', 31, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            \"n_estimators\": trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [-1, 10, 20])\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [5, 10, 20, None]),\n",
    "            \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "            \"subsample\": trial.suggest_categorical(\"subsample\", [0.7, 0.8, 1.0]),\n",
    "            \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [0.7, 0.8, 1.0]),\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": lambda trial: {\n",
    "            \"max_depth\": trial.suggest_categorical('max_depth', [10, 20, 30]),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TimeSeries Cross-validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model_info in models.items():\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = model_info[\"params\"](trial)\n",
    "        model = model_info[\"model\"](**params, random_state=42)\n",
    "\n",
    "        mse_scores = []\n",
    "        \n",
    "        # Perform manual cross-validation using TimeSeriesSplit\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            mse_scores.append(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        return sum(mse_scores) / len(mse_scores)  # Return average MSE\n",
    "    \n",
    "    # Optimize hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest Hyperparameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "    # Train the best model on the full training set\n",
    "    best_model = model_info[\"model\"](**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_test}, index=y_test.index)\n",
    "    pred_csv_path = os.path.join(output_dir, f\"{model_name}_predictions.csv\")\n",
    "    pred_df.to_csv(pred_csv_path, index=True)\n",
    "    \n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        \"Test MSE\": mse_test,\n",
    "        \"Best Hyperparameters\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name} - Mean Squared Error (Test): {mse_test:.4f}\")\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(X_train.columns, best_model.feature_importances_, align='center')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{model_name} Feature Importance Analysis\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Actual vs. Predicted Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.index, y_test, label=\"Actual Predictions\" , linewidth=1)\n",
    "    plt.plot(y_test.index, y_pred_test, label=f\"Predicted with ({model_name})\", color='red' , linestyle='-' , linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{model_name} Time-Series Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "# Save all results to JSON file\n",
    "results_json_path = os.path.join(output_dir, \"summary_results.json\")\n",
    "with open(results_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"{model}: Test MSE = {data['Test MSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define base directory\n",
    "base_dir = r\"C:\\Users\\gkeep\\Desktop\\Results\\Regressors\"\n",
    "\n",
    "# List of neural network folders\n",
    "neural_networks = [\"TCN\", \"LSTM\", \"KAN\", \"TSMixerx\", \"NBEATSx\"]\n",
    "\n",
    "# Initialize empty dictionary\n",
    "mse_data = {}\n",
    "\n",
    "# Load JSON MSEs from each folder\n",
    "for nn_name in neural_networks:\n",
    "    json_path = os.path.join(base_dir, nn_name, \"summary_results.json\")\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        for model, data in results.items():\n",
    "            if model not in mse_data:\n",
    "                mse_data[model] = {}\n",
    "            mse_data[model][nn_name] = data[\"Test MSE\"]\n",
    "\n",
    "# ðŸ”¹ Manually insert ElasticNet results\n",
    "mse_data[\"ElasticNet\"] = {\n",
    "    \"LSTM\": 0.0605757647248136,\n",
    "    \"TCN\": 0.049132745019164016,\n",
    "    \"TSMixerx\": 0.0509752464354896,\n",
    "    \"NBEATSx\": 0.07779817207695296,\n",
    "    \"KAN\": 0.0638408730398479\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "mse_df = pd.DataFrame(mse_data).T\n",
    "\n",
    "# Plot Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(mse_df, annot=True, cmap=\"coolwarm\", fmt=\".3f\", linewidths=0.5)\n",
    "plt.xlabel(\"Neural Networks\")\n",
    "plt.ylabel(\"Surrogate Models\")\n",
    "plt.title(\"MSE Heatmap: Surrogate Models vs. Neural Networks\")\n",
    "plt.tight_layout()\n",
    "# Save the figure before showing it\n",
    "plt.savefig(\n",
    "    r'C:\\Users\\gkeep\\Desktop\\Results\\Regressors\\Surrogate_Heatmap.png',  # Change path + name\n",
    "    dpi=300,             # High resolution (300 dots per inch)\n",
    "    bbox_inches='tight'  # Trim extra whitespace\n",
    ")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trafo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
